{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "A Convolutional Neural Network (CNN) for image classification is made up of multiple layers that extract features, such as edges, corners, etc; and then use a final fully-connected layer to classify objects based on these features. You can visualize this like this:\n",
        "\n",
        "Convolution > Pool > Convolution > Pool > Flatten | > Fully-Connected \n",
        "\n",
        "             Feature Extraction                       |   Classification\n",
        "\n",
        "*Transfer Learning* is a technique where you can take an existing trained model and re-use its feature extraction layers, replacing its final classification layer with a fully-connected layer trained on your own custom images. With this technique, your model benefits from the feature extraction training that was performed on the base model (which may have been based on a larger training dataset than you have access to) to build a classification model for your own specific set of object classes.\n",
        "\n",
        "How does this help? Well, think of it this way. Suppose you take a professional tennis player and a complete beginner, and try to teach them both how to play raquetball. It's reasonable to assume that the professional tennis player will be easier to train, because many of the underlying skills involved in raquetball are already learned. Similarly, a pre-trained CNN model may be easier to train to classify specific set of objects because it's already learned how to identify the features of common objects, such as edges and corners. Fundamentally, a pre-trained model can be a great way to produce an effective classifier even when you have limited data with which to train it.\n",
        "\n",
        "In this notebook, we'll see how to implement transfer learning for a classification model using Tensorflow.\n",
        "\n",
        "## Import libraries\n",
        "\n",
        "First, let's import the Tensorflow libraries we're going to use."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from  tensorflow import keras\n",
        "print('TensorFlow version:',tensorflow.__version__)\n",
        "print('Keras version:',keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Preparing the Data\n",
        "Before we can train the model, we need to prepare the data."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "data_folder = 'data/shapes'\n",
        "# Our source images are 128x128, but the base model we're going to use was trained with 224x224 images\n",
        "pretrained_size = (224,224)\n",
        "batch_size = 15\n",
        "\n",
        "print(\"Getting Data...\")\n",
        "datagen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n",
        "                             validation_split=0.3) # hold back 30% of the images for validation\n",
        "\n",
        "print(\"Preparing training dataset...\")\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_folder,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "print(\"Preparing validation dataset...\")\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    data_folder,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "classnames = list(train_generator.class_indices.keys())\n",
        "print(\"class names: \", classnames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Downloading a trained model to use as a base\n",
        "\n",
        "The ***resnet*** model is an CNN-based image classifier that has been pre-trained using a huge dataset containing thousands of images of many kinds of object. We'll download the trained model, excluding its final linear layer, and freeze the feature extraction layers to retain the trained weights. Then we'll create a fully-connected layer that takes the features extracted by the convolutional layers as an input and generates a prediction probability output for each of our possible classes."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": [
          "outputPrepend"
        ]
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "#Load the base model, not including its final connected layer, and set the input shape to match our images\n",
        "base_model = keras.applications.resnet.ResNet50(weights='imagenet', include_top=False, input_shape=train_generator.image_shape)\n",
        "\n",
        "# Freeze the already-trained layers in the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create layers for classification of our images\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "prediction_layer = Dense(len(classnames), activation='softmax')(x) \n",
        "model = Model(inputs=base_model.input, outputs=prediction_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Now print the full model, which will include the layers of the base model plus the dense layer we added\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "With the layers of the CNN defined, we're ready to train the top layer using our image data."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "# Train the model over 5 epochs\n",
        "num_epochs = 5\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### View the Loss History\n",
        "\n",
        "We tracked average training and validation loss for each epoch. We can plot these to verify that the loss reduced over the training process and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase)."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "epoch_nums = range(1,num_epochs+1)\n",
        "training_loss = history.history[\"loss\"]\n",
        "validation_loss = history.history[\"val_loss\"]\n",
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Evaluate Model Performance\n",
        "We can see the final accuracy based on the test data, but typically we'll want to explore performance metrics in a little more depth. Let's plot a confusion matrix to see how well the model is predicting each class."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Tensorflow doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Generating predictions from validation data...\")\n",
        "# Get the image and label arrays for the first batch of validation data\n",
        "x_test = validation_generator[0][0]\n",
        "y_test = validation_generator[0][1]\n",
        "\n",
        "# Use the moedl to predict the class\n",
        "class_probabilities = model.predict(x_test)\n",
        "\n",
        "# The model returns a probability value for each class\n",
        "# The one with the highest probability is the predicted class\n",
        "predictions = np.argmax(class_probabilities, axis=1)\n",
        "\n",
        "# The actual labels are hot encoded (e.g. [0 1 0], so get the one with the value 1\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classnames))\n",
        "plt.xticks(tick_marks, classnames, rotation=85)\n",
        "plt.yticks(tick_marks, classnames)\n",
        "plt.xlabel(\"Predicted Shape\")\n",
        "plt.ylabel(\"True Shape\")\n",
        "plt.show()"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Using the Trained Model\n",
        "Now that we've trained the model, we can use it to predict the class of an image."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def create_image (size, shape):\n",
        "    from random import randint\n",
        "    import numpy as np\n",
        "    from PIL import Image, ImageDraw\n",
        "    \n",
        "    xy1 = randint(10,40)\n",
        "    xy2 = randint(60,100)\n",
        "    col = (randint(0,200), randint(0,200), randint(0,200))\n",
        "\n",
        "    img = Image.new(\"RGB\", size, (255, 255, 255))\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    if shape == 'circle':\n",
        "        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n",
        "    elif shape == 'triangle':\n",
        "        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n",
        "    else: # square\n",
        "        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n",
        "    del draw\n",
        "    \n",
        "    return np.array(img)\n",
        "\n",
        "# Create a random test image\n",
        "img = create_image ((224,224), classnames[randint(0, len(classnames)-1)])\n",
        "plt.imshow(img)\n",
        "\n",
        "# Modify the image data to match the format of the training features\n",
        "img = np.array(Image.fromarray(img))\n",
        "imgfeatures = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n",
        "imgfeatures = imgfeatures.astype('float32')\n",
        "imgfeatures /= 255\n",
        "\n",
        "# Use the classifier to predict the class\n",
        "predicted_class = model.predict(imgfeatures)\n",
        "# Find the class with the highest predicted probability\n",
        "i = np.where(predicted_class == predicted_class.max())\n",
        "print (classnames[int(i[1])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "## Learning More\n",
        "\n",
        "* [Tensorflow Documentation](https://www.tensorflow.org/tutorials/images/transfer_learning)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}