{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with PyTorch\n",
        "\n",
        "Classical machine learning relies on using statistics to determine relationships between features and labels, and can be very effective for creating predictive models. However, a massive growth in the availability of data coupled with advances in the computing technology required to process it has led to the emergence of new machine learning techniques that mimic the way the brain processes information in a structure called an artificial neural network.\n",
        "\n",
        "## Creating a Neural Network with PyTorch\n",
        "\n",
        "PyTorch is a framework for creating machine learning models, including deep neural networks (DNNs). In this example, we'll use PyTorch to create a simple neural network that classifies penguins into species based on the length and depth of their culmen (bill), their flipper length, and their body mass.\n",
        "\n",
        "> **Citation**: The penguins dataset used in the this exercise is a subset of data collected and made available by [Dr.Â Kristen\n",
        "Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php)\n",
        "and the [Palmer Station, Antarctica LTER](https://pal.lternet.edu/), a\n",
        "member of the [Long Term Ecological Research\n",
        "Network](https://lternet.edu/).\n",
        "\n",
        "### Exploring the Dataset\n",
        "\n",
        "Before we start using PyTorch to create a model, let's load the data we need from the Palmer Islands penguins dataset, which contains observations of three different species of penguin.\n",
        "\n",
        "> **Note**: In reality, you can solve the penguin classification problem easily using classical machine learning techniques without the need for a deep learning model; but it's a useful, easy to understand dataset with which to demonstrate the principles of neural networks in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the training dataset (excluding rows with null values)\n",
        "penguins = pd.read_csv('data/penguins.csv').dropna()\n",
        "\n",
        "# Deep Learning models work best when features are on similar scales\n",
        "# In a real solution, we'd implement some custom normalization for each feature, but to keep things simple\n",
        "# we'll just rescale the FlipperLength and BodyMass so they're on a similar scale to the bill measurements\n",
        "penguins['FlipperLength'] = penguins['FlipperLength']/10\n",
        "penguins['BodyMass'] = penguins['BodyMass']/100\n",
        "\n",
        "# The dataset is too small to be useful for deep learning\n",
        "# So we'll oversample it to triple its size\n",
        "for i in range(1,3):\n",
        "    penguins = penguins.append(penguins)\n",
        "\n",
        "# Display a random sample of 10 observations\n",
        "sample = penguins.sample(10)\n",
        "sample"
      ]
    },
    {
      "source": [
        "The **Species** column is the label our model will predict. Each label value represents a class of penguin species, encoded as 0, 1, or 2. The following code shows the actual species to which these class labels corrrespond."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "penguin_classes = ['Amelie', 'Gentoo', 'Chinstrap']\n",
        "print(sample.columns[0:5].values, 'SpeciesName')\n",
        "for index, row in penguins.sample(10).iterrows():\n",
        "    print('[',row[0], row[1], row[2],row[3], int(row[4]), ']',penguin_classes[int(row[-1])])"
      ]
    },
    {
      "source": [
        "As is common in a supervised learning problem, we'll split the dataset into a set of records with which to train the model, and a smaller set with which to validate the trained model."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features = ['CulmenLength','CulmenDepth','FlipperLength','BodyMass']\n",
        "label = 'Species'\n",
        "   \n",
        "# Split data 70%-30% into training set and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(penguins[features].values,\n",
        "                                                    penguins[label].values,\n",
        "                                                    test_size=0.30,\n",
        "                                                    random_state=0)\n",
        "\n",
        "print ('Training Set: %d, Test Set: %d \\n' % (len(x_train), len(x_test)))\n",
        "print(\"Sample of features and labels:\")\n",
        "\n",
        "# Take a look at the first 25 training features and corresponding labels\n",
        "for n in range(0,24):\n",
        "    print(x_train[n], y_train[n], '(' + penguin_classes[y_train[n]] + ')')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The *features* are the cupmen measurements for each penguin observation, and the *label* is a numeric value that indicates the species of penguin that the observation represents (Amelie, Gentoo, or Chinstrap).\n",
        "\n",
        "### Importing the PyTorch Libraries\n",
        "\n",
        "Since we plan to use PyTorch to create our penguin classifier, we'll need to install and import the PyTorch libraries we intend to use. PyTorch is already installed in your environment. The specific installation of of PyTorch depends on your operating system and whether your computer has graphics processing units (GPUs) that can be used for high-performance processing via *cuda*. You can find detailed instructions at https://pytorch.org/get-started/locally/."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as td\n",
        "\n",
        "# Set random seed for reproducability\n",
        "torch.manual_seed(0)\n",
        "\n",
        "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Prepare the Data for PyTorch\n",
        "PyTorch makes use of *data loaders* to load training and validation data in batches. We've already loaded the data into numpy arrays, but we need to wrap those in PyTorch datasets (in which the data is converted to PyTorch *tensor* objects) and create loaders to read batches from those datasets."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "# Create a dataset and loader for the training data and labels\n",
        "train_x = torch.Tensor(x_train).float()\n",
        "train_y = torch.Tensor(y_train).long()\n",
        "train_ds = td.TensorDataset(train_x,train_y)\n",
        "train_loader = td.DataLoader(train_ds, batch_size=20,\n",
        "    shuffle=False, num_workers=1)\n",
        "\n",
        "# Create a dataset and loader for the test data and labels\n",
        "test_x = torch.Tensor(x_test).float()\n",
        "test_y = torch.Tensor(y_test).long()\n",
        "test_ds = td.TensorDataset(test_x,test_y)\n",
        "test_loader = td.DataLoader(test_ds, batch_size=20,\n",
        "    shuffle=False, num_workers=1)\n",
        "print('Ready to load data..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Define a Neural Network\n",
        "\n",
        "Now we're ready to define our neural network. In this case, we'll create a network that consists of 3 fully-connected layers:\n",
        "* An input layer that receives an input value for each feature (in this case, the length and depth of the penguin's culmen) and applies a *ReLU* activation function.\n",
        "* A hidden layer that receives ten inputs and applies a *ReLU* activation function.\n",
        "* An output layer that uses a *SoftMax* activation function to generate an output for each penguin species (which represent the classification probabilities for each of the three possible penguin species)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "# Number of hidden layer nodes\n",
        "hl = 10\n",
        "\n",
        "# Define the neural network\n",
        "class PenguinNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PenguinNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(len(features), hl)\n",
        "        self.fc2 = nn.Linear(hl, hl)\n",
        "        self.fc3 = nn.Linear(hl, len(penguin_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.softmax(self.fc3(x),dim=1)\n",
        "        return x\n",
        "\n",
        "# Create a model instance from the network\n",
        "model = PenguinNet()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Train the Model\n",
        "To train the model, we need to repeatedly feed the training values forward through the network, use a loss function to calculate the loss, use an optimizer to backpropagate the weight and bias value adjustments, and validate the model using the test data we withheld.\n",
        "\n",
        "To do this, we'll create a function to train and optimize the model, and function to test the model. Then we'll call these functions iteratively over 50 epochs, logging the loss and accuracy statistics for each epoch."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, optimizer):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch, tensor in enumerate(data_loader):\n",
        "        data, target = tensor\n",
        "        #feedforward\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = loss_criteria(out, target)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    #Return average loss\n",
        "    avg_loss = train_loss / (batch+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss\n",
        "           \n",
        "            \n",
        "def test(model, data_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for batch, tensor in enumerate(data_loader):\n",
        "            batch_count += 1\n",
        "            data, target = tensor\n",
        "            # Get the predictions\n",
        "            out = model(data)\n",
        "\n",
        "            # calculate the loss\n",
        "            test_loss += loss_criteria(out, target).item()\n",
        "\n",
        "            # Calculate the accuracy\n",
        "            _, predicted = torch.max(out.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "            \n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss/batch_count\n",
        "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_loss, correct, len(data_loader.dataset),\n",
        "        100. * correct / len(data_loader.dataset)))\n",
        "    \n",
        "    # return average loss for the epoch\n",
        "    return avg_loss\n",
        "\n",
        "# Specify the loss criteria (CrossEntropyLoss for multi-class classification)\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "# (see https://pytorch.org/docs/stable/optim.html#algorithms for details of supported algorithms)\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# We'll track metrics for each epoch in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "# Train over 50 epochs\n",
        "epochs = 50\n",
        "for epoch in range(1, epochs + 1):\n",
        "\n",
        "    # print the epoch number\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "    \n",
        "    # Feed training data into the model to optimize the weights\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    \n",
        "    # Feed the test data into the model to check its performance\n",
        "    test_loss = test(model, test_loader)\n",
        "    \n",
        "    # Log the metrics for this epoch\n",
        "    epoch_nums.append(epoch)\n",
        "    training_loss.append(train_loss)\n",
        "    validation_loss.append(test_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Review Training and Validation Loss\n",
        "After training is complete, we can examine the loss metrics we recorded while training and validating the model. We're really looking for two things:\n",
        "* The loss should reduce with each epoch, showing that the model is learning the right weights and biases to predict the correct labels.\n",
        "* The training loss and validations loss should follow a similar trend, showing that the model is not overfitting to the training data.\n",
        "\n",
        "Let's plot the loss metrics and see:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### View the Learned Weights and Biases\n",
        "The trained model consists of the final weights and biases that were determined by the optimizer during training. Based on our network model we should expect the following values for each layer:\n",
        "* Layer 1: There are two input values going to ten output nodes, so there should be 10 x 2 weights and 10 bias values.\n",
        "* Layer 2: There are ten input values going to ten output nodes, so there should be 10 x 10 weights and 10 bias values.\n",
        "* Layer 3: There are ten input values going to three output nodes, so there should be 3 x 10 weights and 3 bias values."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\n\", model.state_dict()[param_tensor].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model Performance\n",
        "So, is the model any good? The raw accuracy reported from the validation data would seem to indicate that it predicts pretty well; but it's typically useful to dig a little deeper and compare the predictions for each possible class. A common way to visualize the performace of a classification model is to create a *confusion matrix* that shows a crosstab of correct and incorrect predictions for each class."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "#Pytorch doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Set the model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "# Get predictions for the test data\n",
        "x = torch.Tensor(x_test).float()\n",
        "_, predicted = torch.max(model(x).data, 1)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(y_test, predicted.numpy())\n",
        "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(penguin_classes))\n",
        "plt.xticks(tick_marks, penguin_classes, rotation=45)\n",
        "plt.yticks(tick_marks, penguin_classes)\n",
        "plt.xlabel(\"Predicted Species\")\n",
        "plt.ylabel(\"True Species\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The confusion matrix should show a strong diagonal line indicating that there are more correct than incorrect predictions for each class.\n",
        "\n",
        "### Using the Model with New Data\n",
        "\n",
        "Now that we have a model we believe is reasonably accurate, we can use it to predict the species of new penguin observations:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "tags": []
      },
      "cell_type": "code",
      "source": [
        "# Save the model weights\n",
        "model_file = 'models/penguin_classifier.pt'\n",
        "torch.save(model.state_dict(), model_file)\n",
        "del model\n",
        "\n",
        "# New data\n",
        "x_new = [[50.4,15.3,20,50]]\n",
        "print ('New sample: {}'.format(x_new))\n",
        "\n",
        "# Create a new model class and load weights\n",
        "model = PenguinNet()\n",
        "model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Get a prediction for the new data sample\n",
        "x = torch.Tensor(x_new).float()\n",
        "_, predicted = torch.max(model(x).data, 1)\n",
        "\n",
        "print('Prediction:',penguin_classes[predicted.item()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "## Learn More\n",
        "This notebook was designed to help you understand the basic concepts and principles involved in deep neural networks, using a simple PyTorch example. To learn more about PyTorch, take a look at the [tutorials on the PyTorch web site](https://pytorch.org/tutorials/)."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}