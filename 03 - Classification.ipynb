{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "# Classification\n",
        "\n",
        "*Supervised* machine learning techniques involve training a model to operate on a set of *features* and predict a *label* using a dataset that includes some already-known label values. You can think of this function like this, in which ***y*** represents the label we want to predict and ***X*** represents the vector of features the model uses to predict it.\n",
        "\n",
        "$$y = f([x_1, x_2, x_3, ...])$$\n",
        "\n",
        "\n",
        "*Classification* is a form of supervised machine learning in which you train a model to use the features (the ***x*** values in our function) to predict a label (***y***) that calculates the probability of the observed case belonging to each of a number of possible classes, and predicting an appropriate label. The simplest form of classification is *binary* classification, in which the label is 0 or 1, representing one of two classes; for example, \"True\" or \"False\"; \"Internal\" or \"External\"; \"Profitable\" or \"Non-Profitable\"; and so on. "
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Explore Data for Binary Classification\n",
        "Let's start by looking at an example of *binary classification*.\n",
        "We'll train a binary classifier to predict whether or not a patient should be tested for diabetes based on some medical data. Run the following cell to load a CSV file of patent data into a **Pandas** dataframe:"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the training dataset\n",
        "diabetes = pd.read_csv('data/diabetes.csv')\n",
        "diabetes.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "This data consists of diagnostic information about some patients who have been tested for diabetes. Scroll to the right if necessary, and note that the final column in the dataset (**Diabetic**) contains the value ***0*** for patients who tested negative for diabetes, and ***1*** for patients who tested positive. This is the label that we will train our mode to predict; most of the other columns (**Pregnancies**,**PlasmaGlucose**,**DiastolicBloodPressure**, and so on) are the features we will use to predict the **Diabetic** label.\n",
        "\n",
        "Let's separate the features from the labels - we'll call the features ***X*** and the label ***y***:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Separate features and labels\n",
        "features = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n",
        "label = 'Diabetic'\n",
        "X, y = diabetes[features].values, diabetes[label].values\n",
        "\n",
        "for n in range(0,4):\n",
        "    print(\"Patient\", str(n+1), \"\\n  Features:\",list(X[n]), \"\\n  Label:\", y[n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Now let's compare the feature distributions for each label value."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "features = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n",
        "for col in features:\n",
        "    diabetes.boxplot(column=col, by='Diabetic', figsize=(6,6))\n",
        "    plt.title(col)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "For some of the features, there's a noticable difference in the distribution for each label value. In particular, **Pregnancies** and **Age** show markedly different distributions for diabetic patients than for non-diabetic patients. These features may help predict whether or not a patient is diabetic.\n",
        "\n",
        "### Split the data\n",
        "\n",
        "Our dataset includes known values for the label, so we can use this to train a classifier so that it finds a statistical relationship between the features and the label value; but how will we know if our model is any good? How do we know it will predict correctly when we use it with new data that it wasn't trained with? Well, we can take advantage of the fact we have a large dataset with known label values, use only some of it to train the model, and hold back some to test the trained model - enabling us to compare the predicted labels with the already known labels in the test set.\n",
        "\n",
        "In Python, the **scikit-learn** package contains a large number of functions we can use to build a machine learning model - including a **train_test_split** function that ensures we get a statistically random split of training and test data. We'll use that to split the data into 70% for training and hold back 30% for testing."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data 65%-35% into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "print ('Training cases: %d\\nTest cases: %d' % (X_train.size, X_test.size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate a Binary Classification Model\n",
        "OK, now we're ready to train our model by fitting the training features (**X_train**) to the training labels (**y_train**). There are various algorithms we can use to train the model. In this example, we'll use *Logistic Regression*, which is a well-established algorithm for classification. In addition to the training features and labels, we'll need to set a *regularization* parameter. This is used to counteract any bias in the sample, and help the model generalize well by avoiding *overfitting* the model to the training data.\n",
        "\n",
        "> **Note**: Parameters for machine learning algorithms are generally referred to as *hyperparameters*."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Set regularization rate\n",
        "reg = 0.01\n",
        "\n",
        "# train a logistic regression model on the training set\n",
        "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
        "print (model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Now we've trained the model using the training data, we can use the test data we held back to evaluate how well it predicts. Again, **scikit-learn** can help us do this. Let's start by using the model to predict labels for our test set, and compare the predicted labels to the known labels:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)\n",
        "print('Predicted labels: ', predictions)\n",
        "print('Actual labels:    ' ,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The arrays of labels are too long to be displayed in the notebook output, so we can only compare a few values. Even if we printed out all of the predicted and actual labels, there are too many of them to make this a sensible way to evaluate the model. Fortunately, **scikit-learn** has a few more tricks up its sleeve, and it provides some metrics that we can use to evaluate the model.\n",
        "\n",
        "The most obvious thing you might want to do is to check the *accuracy* of the predictions - in simple terms, what proportion of the labels did the model predict correctly?"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Accuracy: ', accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The accuracy is returned as a decimal value - a value of 1.0 would mean that the model got 100% of the predictions right; while an accuracy of 0.0 is, well, pretty useless!\n",
        "\n",
        "Accuracy seems like a sensible metric to evaluate (and to a certain extent it is), but you need to be careful about drawing too many conclusions from the accuracy of a classifier. Remember that it's simply a measure of how many cases were predicted correctly. Suppose only 3% of the population is diabetic. You could create a classifier that always just predicts 0, and it would be 97% accurate - but not terribly helpful in identifying patients with diabetes!\n",
        "\n",
        "Fortunately, there are some other metrics that reveal a little more about how our model is performing."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn. metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The classification report includes the following metrics:\n",
        "* *Precision*: The proportion of *positive* (1) predictions that were in fact positive.\n",
        "* *Recall*: The proportion of actual positive cases that the classifier correctly identified.\n",
        "* *F1-Score*: An average metric that takes both precision and recall into account.\n",
        "* *Support*: A weighted average of prevelance for the two classes.\n",
        "\n",
        "The precision and recall metrics are derived from four core metrics:\n",
        "* *True Positives*: The predicted label and the actual label are both 1.\n",
        "* *False Positives*: The predicted label is 1, but the actual label is 0.\n",
        "* *False Negatives*: The predicted label is 0, but the actual label is 1.\n",
        "* *True Negatives*: The predicted label and the actual label are both 0.\n",
        "\n",
        "These metrics are generally shown together as a *confusion matrix*, which takes the following form:\n",
        "\n",
        "<table style=\"border: 1px solid black;\">\n",
        "    <tr style=\"border: 1px solid black;\">\n",
        "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FP</td>\n",
        "    </tr>\n",
        "    <tr style=\"border: 1px solid black;\">\n",
        "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TP</td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "Note that the correct (*true*) predictions form a diagonal line from top left to bottom right - these figures should be significantly higher than the *false* predictions if the model is any good.\n",
        "\n",
        "In Python, you can use the **sklearn.metrics.confusion_matrix** function to find these values for a trained classifier:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Print the confusion matrix\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "print (cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Up until now, we've considered the predictions from the model as being either a 1 or a 0. Actually, things are a little more complex than that. Statistical machine learning algorithms, like logistic regression, are based on *probability*; so what actually gets predicted by a binary classifier is the probability that the label is true (**P(y)**) and the probability that the label is false (1 - **P(y)**). A threshold value of 0.5 is used to decide whether the predicted label is a 1 (*P(y) > 0.5*) or a 0 (*P(y) <= 0.5*). You can use the **predict_proba** method to see the probability pairs for each case:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "y_scores = model.predict_proba(X_test)\n",
        "print(y_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The decision to score a prediction as a 1 or a 0 depends on the threshold to which the predicted probabilties are compared. If we were to change the threshold, it would affect the predictions; and therefore change the metrics in the confusion matrix. A common way to evaluate a classifier is to examine the *true positive rate* (which is another name for recall) and the *false positive rate* for a range of possible thresholds. These rates are then plotted against all possible thresholds to form a chart known as a *received operator characteristic (ROC) chart*, like this:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "\n",
        "# plot ROC curve\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "# Plot the diagonal 50% line\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "# Plot the FPR and TPR achieved by our model\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The ROC chart shows the curve of the true and false positive rates for different threshold values between 0 and 1. A perfect classifier would have a curve that goes straight up the left side and straight across the top. The diagonal line across the chart represents the probability of predicting correctly with a 50/50 random prediction; so you obviously want the curve to be higher than that (or your model is no better than simply guessing!).\n",
        "\n",
        "The area under the curve (AUC) is a value between 0 and 1 that quantifies the overall performance of the model. The closer to 1 this value is, the better the model. Once again, Scikit-Learn includes a function to calculate this metric."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "In this case, the ROC curve and its AUC indicate that the model performs better than a random guess.\n",
        "\n",
        "### Use the Model for Inferencing\n",
        "Now that we have a reasonably useful trained model, we can save it for use later to predict labels for new data:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model as a pickle file\n",
        "filename = './models/diabetes_model.pkl'\n",
        "joblib.dump(model, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "When we have some new observations for which the label is unknown, we can load the model and use it to predict values for the unknown label:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Load the model from the file\n",
        "model = joblib.load(filename)\n",
        "\n",
        "# predict on a new sample\n",
        "# The model accepts an array of feature arrays (so you can predict the classes of multiple patients in a single call)\n",
        "# We'll create an array with a single array of features, representing one patient\n",
        "X_new = [[2,180,74,24,21,23.9091702,1.488172308,22]]\n",
        "print ('New sample: {}'.format(list(X_new[0])))\n",
        "\n",
        "# Get a prediction\n",
        "pred = model.predict(X_new)\n",
        "\n",
        "# The model returns an array of predictions - one for each set of features submitted\n",
        "# In our case, we only submitted one patient, so our prediction is the first one in the resulting array.\n",
        "print('Predicted class is {}'.format(pred[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Train a Multiclass Classification Model\n",
        "Binary classification techniques work well when the data observations belong to one of two classes or categories, such as \"True\" or \"False\". When the data can be categorized into more than two classes, you must use a multiclass classification algorithm.\n",
        "\n",
        "Fortunately, in most machine learning frameworks, including scikit-learn, implementing a multiclass classifier is not significantly more complex than binary classification - and in many cases, the classification algorithm classes used for binary classification implicitly support multiclass classification.\n",
        "\n",
        "Let's start by examining a dataset that contains observations of multiple classes. We'll use one of the most commonly used examples in machine learning - the ***Iris*** dataset, in which characteristics of iris flowers are recorded along with the specific species of iris.\n",
        "\n",
        "This dataset is so commonly used in machine learning examples, it's available directly from the scikit-learn library. Run the following cell to load it:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The dataset in scikit-learn consists of:\n",
        "* A description of the dataset\n",
        "* An array named **data** containing the feature values\n",
        "* An array named **feature_names** containing the names of the features (*sepal length*, *sepal width*, *petal length*, and *petal width*).\n",
        "* An array named **target** containing the corresponding labels\n",
        "* An array named **target_names** containing the species names that correspond to each possible label value (*setosa*, *versicolor*, and *virginica*).\n",
        "\n",
        "Let's combine the features and label values into a dataframe and look at a sample of them:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "features = pd.DataFrame(data = np.c_[iris.data,iris.target], columns = iris.feature_names + ['label'])\n",
        "features.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The labels in the dataset are 0, 1, and 2. Let's see what those labels correspond to in terms of species names:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "print(iris.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "As with binary classification, we need to split the multiclass data into a set of features and labels for training, and a second set of features and labels for testing the trained model. The dataset from scikit-learn already includes separated features (**iris.data**) and labels (**iris.target**), so we just need to separate these into training and test sets:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data 70%-30% into training set and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.30, random_state=0)\n",
        "\n",
        "print ('Training Set: %d, Test Set: %d \\n' % (x_train.size, x_test.size))\n",
        "\n",
        "print(\"Sample of features and labels:\")\n",
        "# Take a look at the first 10 training features and corresponding labels\n",
        "for n in range(0,9):\n",
        "    print(x_train[n], y_train[n], '(' + iris.target_names[y_train[n]] + ')')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Now that we have a set of training features and corresponding training labels, we can fit a multiclass classification algorithm to the data to create a model. Most scikit-learn classification algorithms inherently supports multiclass classification. In this example, we'll use another common algorithm called *Random Forest*, which is an example of an *ensemble* algorithm that combines the results of multiple base algorithms:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# train a random forest model on the training set\n",
        "multi_model = RandomForestClassifier(n_estimators=100).fit(x_train, y_train)\n",
        "print (multi_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Evaluate the Multiclass Classifier\n",
        "Let's start by predicting the labels for the test features, and comparing the predicted labels to the actual labels:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "iris_predictions = multi_model.predict(x_test)\n",
        "print('Predicted labels: ', iris_predictions[:15])\n",
        "print('Actual labels   : ' ,y_test[:15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Looks pretty good. What's the overall *accuracy* of the model when used with the test dataset?"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Accuracy: ', accuracy_score(y_test, iris_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "OK, how about some other metrics? We can calculate the *precision*, *recall*, and *f1-score* for each class:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn. metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, iris_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Now let's look at the confusion matrix for our model:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Print the confusion matrix\n",
        "mcm = confusion_matrix(y_test, iris_predictions)\n",
        "print(mcm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Note that the confusion matrix shows the intersection of predicted and actual label values for each class - in simple terms, the diagonal intersections from top-left to bottom-right indicate the number of correct predictions.\n",
        "\n",
        "It's generally more intuitive to visualize this as a heat map, like this:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(mcm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(iris.target_names))\n",
        "plt.xticks(tick_marks, iris.target_names, rotation=45)\n",
        "plt.yticks(tick_marks, iris.target_names)\n",
        "plt.xlabel(\"Predicted Species\")\n",
        "plt.ylabel(\"True Species\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Use the model with new data observations\n",
        "As before, let's save our trained model so we can use it again later."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model as a pickle file\n",
        "filename = './models/iris_model.pkl'\n",
        "joblib.dump(multi_model, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "OK, so now we have a trained model. Let's use it to predict the class of a new iris observation:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Load the model from the file\n",
        "multi_model = joblib.load(filename)\n",
        "\n",
        "# The model accepts an array of feature arrays (so you can predict the classes of multiple iris observations in a single call)\n",
        "# We'll create an array with a single array of features, representing one iris\n",
        "x_new = [[6.6,3.2,5.8,2.4]]\n",
        "print ('New sample: {}'.format(x_new[0]))\n",
        "\n",
        "# The model returns an array of predictions - one for each set of features submitted\n",
        "# In our case, we only submitted one iris, so our prediction is the first one in the resulting array.\n",
        "iris_pred = multi_model.predict(x_new)[0]\n",
        "print('Predicted class is', iris.target_names[iris_pred])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "You can also submit a batch of iris observations to the model, and get back a prediction for each one."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# This time our input is an array of two feature arrays\n",
        "x_new = [[6.6,3.2,5.8,2.4],\n",
        "         [5.6, 2.5, 3.9, 1.1]]\n",
        "print ('New sample: {}'.format(x_new))\n",
        "\n",
        "# Call the web service, passing the input data\n",
        "predictions = multi_model.predict(x_new)\n",
        "\n",
        "# Get the predicted classes.\n",
        "for prediction in predictions:\n",
        "    print(prediction, '(' + iris.target_names[prediction] +')')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "In this notebook, you created a binary classification model and a multiclass classification model."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}